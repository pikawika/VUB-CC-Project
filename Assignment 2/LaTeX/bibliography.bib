@online{latex_template,
	title        = {VUB LaTeX huisstijl},
	author       = {De Smet, Ruben},
	year         = 2020,
	url          = {https://gitlab.com/rubdos/texlive-vub},
	urldate      = {2020-11-02},
	note         = {GitHub commit: d91f55799abd390a7dac92492f894b9b5fea2f47}
}
@online{github_project,
	title        = {Computational Creativity project},
	author       = {Bontinck, Lennert},
	year         = 2021,
	url          = {https://github.com/pikawika/VUB-CC-Project},
	urldate      = {2021-03-07},
	note         = {GitHub commit: TODO}
}
@inproceedings{gan_founder,
	title        = {Generative Adversarial Nets},
	author       = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year         = 2014,
	booktitle    = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
	location     = {Montreal, Canada},
	publisher    = {MIT Press},
	address      = {Cambridge, MA, USA},
	series       = {NIPS'14},
	pages        = {2672–2680},
	abstract     = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	numpages     = 9
}
@online{gancar1,
	title        = {Using Neural Nets to Design Cars},
	author       = {Soomar, Ali and Balaji, Rohan and McCray, Ryan and Hung, Elvin and Guggari, Sandeep and Cleaver, Nathan},
	year         = 2020,
	url          = {https://medium.com/@alisoomar/adversarial-networks-for-car-image-generation-9bdf5977bec8},
	urldate      = {2021-03-08}
}
@online{gancar2,
	title        = {GAN to Generate Images of Cars},
	author       = {Amitesh, Om},
	year         = 2020,
	url          = {https://medium.com/swlh/gan-to-generate-images-of-cars-5f706ca88da},
	urldate      = {2021-03-08}
}
@online{gancar3,
	title        = {Using GANs to Generate Images of Race Cars},
	author       = {ODSC, Team},
	year         = 2019,
	url          = {https://opendatascience.com/using-gans-to-generate-images-of-race-cars/},
	urldate      = {2021-03-08}
}
@article{cardb,
	title        = {LSUN-Stanford Car Dataset: Enhancing Large-Scale Car Image Datasets Using Deep Learning for Usage in GAN Training},
	author       = {Kramberger, Tin and Potočnik, Božidar},
	year         = 2020,
	journal      = {Applied Sciences},
	volume       = 10,
	number       = 14,
	doi          = {10.3390/app10144913},
	issn         = {2076-3417},
	url          = {https://www.mdpi.com/2076-3417/10/14/4913},
	article-number = 4913
}
@article{scientificcargan1,
	title        = {FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery},
	author       = {Krishna Kumar Singh and Utkarsh Ojha and Yong Jae Lee},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1811.11155},
	url          = {http://arxiv.org/abs/1811.11155},
	archiveprefix = {arXiv},
	eprint       = {1811.11155},
	timestamp    = {Fri, 30 Nov 2018 12:44:28 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1811-11155.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{scientificcargan2,
	title        = {Analyzing and Improving the Image Quality of StyleGAN},
	author       = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
	year         = 2019,
	journal      = {CoRR},
	volume       = {abs/1912.04958},
	url          = {http://arxiv.org/abs/1912.04958},
	archiveprefix = {arXiv},
	eprint       = {1912.04958},
	timestamp    = {Thu, 02 Jan 2020 18:08:18 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1912-04958.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{stylegan,
	title        = {A Style-Based Generator Architecture for Generative Adversarial Networks},
	author       = {Tero Karras and Samuli Laine and Timo Aila},
	year         = 2018,
	journal      = {CoRR},
	volume       = {abs/1812.04948},
	url          = {http://arxiv.org/abs/1812.04948},
	archiveprefix = {arXiv},
	eprint       = {1812.04948},
	timestamp    = {Tue, 01 Jan 2019 15:01:25 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1812-04948.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{creativecargan,
	title        = {Creative Intelligence - Automating Car Design Studio with Generative Adversarial Networks (GAN)},
	author       = {Radhakrishnan, Sreedhar and Bharadwaj, Varun and Manjunath, Varun and Srinath, Ramamoorthy},
	year         = 2018,
	booktitle    = {Machine Learning and Knowledge Extraction},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {160--175},
	isbn         = {978-3-319-99740-7},
	editor       = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	abstract     = {In this paper, we propose and implement a system based on Generative Adversarial Networks (GANs), to create novel car designs from a minimal design studio sketch. A key component of our architecture is a novel convolutional filter layer, that produces sketches similar to those drawn by designers during rapid prototyping. The sketches produced are more aesthetic than the ones from standard edge detection filters or gradient operations. In addition, we show that our system is able to generate hitherto unseen perspectives of a car, given a sketch of the car at just a single viewing angle. For extensive training, testing and validation of our system, we have developed a comprehensive, paired dataset of around 100,000 car images (with transparent backgrounds) and their respective sketches. Our work augments human intelligence and creativity using machine learning and deep neural networks. Our system has the significant benefit of reducing the cycle time in the sketch-to-image process which has largely been considered a creative domain. This is achieved by learning to interpret a preliminary sketch drawn by a designer, to generate novel visual designs in a matter of seconds, which may otherwise require considerable time and effort. While the system enhances the productivity of the designer, the machine learning enhanced design visualizations can cut costs during the product prototyping stage. Our system exhibits good impactful potential for the automobile industry and can be easily adapted to industries which require creative intelligence.}
}
@misc{rigstylegan,
	title        = {StyleRig: Rigging StyleGAN for 3D Control over Portrait Images},
	author       = {Ayush Tewari and Mohamed Elgharib and Gaurav Bharaj and Florian Bernard and Hans-Peter Seidel and Patrick Pérez and Michael Zollhöfer and Christian Theobalt},
	year         = 2020,
	eprint       = {2004.00121},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@article{CNNrigging,
	title        = {Understanding the role of individual units in a deep neural network},
	author       = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
	year         = 2020,
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {National Academy of Sciences},
	volume       = 117,
	number       = 48,
	pages        = {30071--30078},
	doi          = {10.1073/pnas.1907375117},
	issn         = {0027-8424},
	url          = {https://www.pnas.org/content/117/48/30071},
	eprint       = {https://www.pnas.org/content/117/48/30071.full.pdf}
}
